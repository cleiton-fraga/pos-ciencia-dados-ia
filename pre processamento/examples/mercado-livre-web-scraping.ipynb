{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d0e3e2",
   "metadata": {},
   "source": [
    "# Web Scraping Mercado Livre\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T14:21:32.058019Z",
     "start_time": "2026-02-10T14:20:11.854153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Web scraping Mercado Livre (busca)\n",
    "# Obs.: o Mercado Livre muda o HTML com frequência; por isso, priorizamos JSON-LD (SEO) como fonte principal.\n",
    "\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "DEFAULT_HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "\n",
    "\n",
    "def fetch_html(url, session=None, timeout=20, retries=3, backoff_s=1.5):\n",
    "    session = session or requests.Session()\n",
    "    last_exc = None\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            resp = session.get(url, headers=DEFAULT_HEADERS, timeout=timeout)\n",
    "\n",
    "            # rate limit / manutenção\n",
    "            if resp.status_code in (429, 503):\n",
    "                time.sleep(backoff_s * (attempt + 1))\n",
    "                continue\n",
    "\n",
    "            resp.raise_for_status()\n",
    "            resp.encoding = resp.apparent_encoding or resp.encoding\n",
    "            return resp.text\n",
    "        except requests.RequestException as exc:\n",
    "            last_exc = exc\n",
    "            time.sleep(backoff_s * (attempt + 1))\n",
    "\n",
    "    raise RuntimeError(f\"Falha ao baixar a página: {url}. Erro: {last_exc}\")\n",
    "\n",
    "\n",
    "def build_search_url(query, offset=1):\n",
    "    slug = quote_plus(query).replace(\"+\", \"-\")\n",
    "    base = f\"https://lista.mercadolivre.com.br/{slug}\"\n",
    "    if offset and offset > 1:\n",
    "        return f\"{base}_Desde_{offset}\"\n",
    "    return base\n",
    "\n",
    "\n",
    "def _to_number_br(value):\n",
    "    if value is None:\n",
    "        return None\n",
    "    if isinstance(value, (int, float)):\n",
    "        return float(value)\n",
    "\n",
    "    s = str(value)\n",
    "    s = re.sub(r\"[^0-9,\\.]\", \"\", s)\n",
    "\n",
    "    # Formato comum no BR: 1.234,56\n",
    "    if s.count(\",\") == 1 and s.count(\".\") >= 1:\n",
    "        s = s.replace(\".\", \"\").replace(\",\", \".\")\n",
    "    else:\n",
    "        s = s.replace(\",\", \".\")\n",
    "\n",
    "    try:\n",
    "        return float(s)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_products_from_jsonld(soup):\n",
    "    products = []\n",
    "\n",
    "    for script in soup.find_all(\"script\", attrs={\"type\": \"application/ld+json\"}):\n",
    "        raw = (script.string or script.get_text() or \"\").strip()\n",
    "        if not raw:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            data = json.loads(raw)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        candidates = data if isinstance(data, list) else [data]\n",
    "        for obj in candidates:\n",
    "            if not isinstance(obj, dict):\n",
    "                continue\n",
    "\n",
    "            item_list = obj.get(\"itemListElement\")\n",
    "            if not isinstance(item_list, list):\n",
    "                continue\n",
    "\n",
    "            for el in item_list:\n",
    "                item = el.get(\"item\") if isinstance(el, dict) else None\n",
    "                if not isinstance(item, dict):\n",
    "                    continue\n",
    "\n",
    "                offers = item.get(\"offers\") if isinstance(item.get(\"offers\"), dict) else {}\n",
    "\n",
    "                products.append(\n",
    "                    {\n",
    "                        \"TITLE\": item.get(\"name\"),\n",
    "                        \"PRICE\": _to_number_br(offers.get(\"price\")),\n",
    "                        \"URL\": item.get(\"url\")\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Dedup por URL (ou título se faltar URL)\n",
    "    dedup = {}\n",
    "    for p in products:\n",
    "        key = p.get(\"URL\") or p.get(\"TITLE\")\n",
    "        if key and key not in dedup:\n",
    "            dedup[key] = p\n",
    "    return list(dedup.values())\n",
    "\n",
    "\n",
    "def parse_products_from_dom(soup):\n",
    "    products = []\n",
    "    items = soup.select(\"li.ui-search-layout__item, li.ui-search-layout__stack\") or []\n",
    "\n",
    "    for it in items:\n",
    "        title_el = it.select_one(\n",
    "            \"h2.ui-search-item__title, h2.poly-component__title, a.poly-component__title\"\n",
    "        )\n",
    "        if not title_el:\n",
    "            continue\n",
    "\n",
    "        title = title_el.get_text(strip=True)\n",
    "\n",
    "        link_el = it.select_one(\"a.ui-search-link, a.poly-component__title\")\n",
    "        url = link_el.get(\"href\") if link_el else None\n",
    "\n",
    "        price_el = it.select_one(\n",
    "            \"span.andes-money-amount__fraction, div.andes-money-amount-combo__main-container\"\n",
    "        )\n",
    "        price = _to_number_br(price_el.get_text(\" \", strip=True) if price_el else None)\n",
    "\n",
    "        products.append({\"TITLE\": title, \"PRICE\": price, \"URL\": url})\n",
    "\n",
    "    return products\n",
    "\n",
    "\n",
    "def _first_dict(value):\n",
    "    if isinstance(value, dict):\n",
    "        return value\n",
    "    if isinstance(value, list) and value and isinstance(value[0], dict):\n",
    "        return value[0]\n",
    "    return {}\n",
    "\n",
    "\n",
    "def parse_product_details_from_jsonld(soup):\n",
    "    \"\"\"Extrai dados mínimos do JSON-LD da página do produto.\"\"\"\n",
    "    product = None\n",
    "    breadcrumbs = None\n",
    "\n",
    "    for script in soup.find_all(\"script\", attrs={\"type\": \"application/ld+json\"}):\n",
    "        raw = (script.string or script.get_text() or \"\").strip()\n",
    "        if not raw:\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(raw)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        candidates = data if isinstance(data, list) else [data]\n",
    "        for obj in candidates:\n",
    "            if not isinstance(obj, dict):\n",
    "                continue\n",
    "\n",
    "            t = obj.get(\"@type\")\n",
    "            if t == \"Product\" and product is None:\n",
    "                product = obj\n",
    "            if t == \"BreadcrumbList\" and breadcrumbs is None:\n",
    "                breadcrumbs = obj\n",
    "\n",
    "    details = {\"CATEGORY\": None, \"RATING\": None, \"IN_STOCK\": None, \"AVAILABILITY\": None}\n",
    "\n",
    "    # Categoria: usa breadcrumb quando existir\n",
    "    if isinstance(breadcrumbs, dict):\n",
    "        elements = breadcrumbs.get(\"itemListElement\")\n",
    "        if isinstance(elements, list) and elements:\n",
    "            names = []\n",
    "            for el in elements:\n",
    "                if not isinstance(el, dict):\n",
    "                    continue\n",
    "                item = el.get(\"item\")\n",
    "                if isinstance(item, dict) and item.get(\"name\"):\n",
    "                    names.append(str(item.get(\"name\")).strip())\n",
    "            # normalmente o último é o próprio produto; remove se houver mais de 1\n",
    "            if len(names) >= 2:\n",
    "                names = names[:-1]\n",
    "            details[\"CATEGORY\"] = \" > \".join(names) if names else None\n",
    "\n",
    "    # fallback: Product.category (quando existir)\n",
    "    if details[\"CATEGORY\"] is None and isinstance(product, dict):\n",
    "        cat = product.get(\"category\")\n",
    "        if isinstance(cat, str) and cat.strip():\n",
    "            details[\"CATEGORY\"] = cat.strip()\n",
    "\n",
    "    # Avaliação\n",
    "    if isinstance(product, dict):\n",
    "        ar = product.get(\"aggregateRating\")\n",
    "        if isinstance(ar, dict):\n",
    "            details[\"RATING\"] = _to_number_br(ar.get(\"ratingValue\"))\n",
    "\n",
    "    # Disponibilidade\n",
    "    if isinstance(product, dict):\n",
    "        offers = _first_dict(product.get(\"offers\"))\n",
    "        avail = offers.get(\"availability\")\n",
    "        if isinstance(avail, str) and avail:\n",
    "            details[\"AVAILABILITY\"] = avail\n",
    "            if avail.endswith(\"InStock\"):\n",
    "                details[\"IN_STOCK\"] = True\n",
    "            elif avail.endswith(\"OutOfStock\"):\n",
    "                details[\"IN_STOCK\"] = False\n",
    "\n",
    "    return details\n",
    "\n",
    "\n",
    "def enrich_with_details(df, max_items=30, sleep_s=1.0):\n",
    "    \"\"\"Visita as páginas dos produtos para coletar categoria/avaliação/estoque.\"\"\"\n",
    "    if df.empty or \"URL\" not in df.columns:\n",
    "        return df\n",
    "\n",
    "    session = requests.Session()\n",
    "    urls = [u for u in df[\"URL\"].dropna().unique().tolist() if isinstance(u, str) and u.startswith(\"http\")]\n",
    "    if max_items is not None:\n",
    "        urls = urls[: int(max_items)]\n",
    "\n",
    "    details_by_url = {}\n",
    "    for url in urls:\n",
    "        html = fetch_html(url, session=session)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        details_by_url[url] = parse_product_details_from_jsonld(soup)\n",
    "        time.sleep(sleep_s)\n",
    "\n",
    "    details_df = pd.DataFrame.from_dict(details_by_url, orient=\"index\").reset_index().rename(columns={\"index\": \"URL\"})\n",
    "    out = df.merge(details_df, on=\"URL\", how=\"left\")\n",
    "    out[\"RATING\"] = pd.to_numeric(out.get(\"RATING\"), errors=\"coerce\")\n",
    "    ##out[\"IN_STOCK\"] = pd.to_numeric(out.get(\"IN_STOCK\"), errors=\"coerce\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def scrape_search(query, pages=1, sleep_s=1.0):\n",
    "    session = requests.Session()\n",
    "    all_products = []\n",
    "\n",
    "    for page in range(pages):\n",
    "        offset = 1 + page * 50\n",
    "        url = build_search_url(query, offset=offset)\n",
    "\n",
    "        html = fetch_html(url, session=session)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        products = parse_products_from_jsonld(soup) or parse_products_from_dom(soup)\n",
    "        if not products:\n",
    "            raise RuntimeError(\n",
    "                \"Não foi possível extrair produtos desta página. \"\n",
    "                \"O HTML pode ter mudado ou a requisição foi bloqueada.\"\n",
    "            )\n",
    "\n",
    "        for p in products:\n",
    "            p[\"SOURCE_URL\"] = url\n",
    "        all_products.extend(products)\n",
    "\n",
    "        time.sleep(sleep_s)\n",
    "\n",
    "    df = pd.DataFrame(all_products)\n",
    "    df[\"SCRAPY_DATETIME\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    df[\"PRICE\"] = pd.to_numeric(df[\"PRICE\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"TITLE\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "termo = \"tablet\"\n",
    "# Exemplo: buscar por termo e paginar\n",
    "df_mercado_livre = scrape_search(query=termo, pages=2, sleep_s=2.0)\n",
    "\n",
    "# Enriquecimento (categoria, avaliação e disponibilidade) via páginas de detalhe\n",
    "df_mercado_livre = enrich_with_details(df_mercado_livre, max_items=30, sleep_s=1.0)\n",
    "\n",
    "# Dados mínimos solicitados\n",
    "df_mercado_livre[[\"TITLE\", \"PRICE\", \"CATEGORY\", \"RATING\", \"IN_STOCK\"]].head(10)\n",
    "\n",
    "# Salva o DataFrame em um arquivo JSON (um registro por objeto)\n",
    "# Usa date_format='iso' para serializar datetimes e force_ascii=False para preservar acentuação\n",
    "import os\n",
    "out_path = os.path.join('../pre processamento', f\"mercado_livre_{termo}_20260210.json\")\n",
    "try:\n",
    "    # tenta salvar diretamente com pandas (mais rápido)\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    df_mercado_livre.to_json(out_path, orient='records', date_format='iso', force_ascii=False)\n",
    "    print(f'Arquivo salvo: {os.path.abspath(out_path)} ({len(df_mercado_livre)} registros)')\n",
    "except Exception as e:\n",
    "    # fallback: converter para dict e usar json.dump para maior controle\n",
    "    import json as _json\n",
    "    records = df_mercado_livre.where(pd.notnull(df_mercado_livre), None).to_dict(orient='records')\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    with open(out_path, 'w', encoding='utf-8') as fh:\n",
    "        _json.dump(records, fh, ensure_ascii=False, indent=2)\n",
    "    print(f'Arquivo salvo via fallback: {os.path.abspath(out_path)} ({len(records)} registros)')\n"
   ],
   "id": "9003155b663c74b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo: /Users/cleiton-fraga/PycharmProjects/pos-ciencia-dados-ia/pre processamento/pre processamento/mercado_livre_caixa de som_20260210.json (110 registros)\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
